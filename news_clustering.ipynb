{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Clustering sobre noticias del periodico: El Tiempo (COL)***\n",
    "\n",
    "Deseamos saber los temas de las noticias, asi como un resumen de estos en busca de una actualizacion via este periodo Colombiano. Para esto vamos a seguir estos pasos:\n",
    "\n",
    "1. Obtencion de urls de las noticias.\n",
    "2. Obtencion del texto de la noticia.\n",
    "3. Generacion de embeddings de las noticias con [`Ollama`](https://ollama.com/) y la libreria [`langchain`](https://python.langchain.com/docs/introduction/).\n",
    "4. Metodos de clustering.\n",
    "5. Uso de LLM local (en Colab con Ollama) para obtener resumenes, insight y caractetizaciones de los cluster formados.\n",
    "6. Discusion de resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Obtener urls de noticias del Tiempo***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_new = (\n",
    "    \"\"\"https://www.eltiempo.com/deportes/otros-deportes/david-alonso-imparable-en-moto3-gano-en-indonesia-y-podria-asegurar\"\"\"\n",
    "    \"\"\"-el-titulo-en-la-proxima-carrera-en-japon-3385519\"\"\"\n",
    ")\n",
    "\n",
    "session = requests.Session()\n",
    "# Peticion HTTP\n",
    "response = session.get(url_new)\n",
    "# Respuesta peticion\n",
    "print(\"Respuesta a la peticion url:\", response.status_code)\n",
    "print(f\"{100 * '='}\")\n",
    "# Contenido de la pagina web\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.eltiempo.com/sitemap-articles-current.xml'\n",
    "session = requests.Session()\n",
    "response = session.get(url)\n",
    "if response.raise_for_status:\n",
    "    print(f\"OK, with the page: {url}. Status Code: {response.status_code}\\n\")\n",
    "    print(f\"Text from Page:\")\n",
    "    print(response.text[: 1000])\n",
    "else:\n",
    "    print(f\"Problem in the page: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = {\n",
    "    \"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\",\n",
    "    \"news\": \"http://www.google.com/schemas/sitemap-news/0.9\",\n",
    "    \"video\": \"http://www.google.com/schemas/sitemap-video/1.1\"\n",
    "}\n",
    "\n",
    "# Hacemos lectura del texto XML\n",
    "df_urls_news_el_tiempo = pd.read_xml(StringIO(response.text), xpath=\".//ns:url\", namespaces=namespaces)\n",
    "df_urls_news_el_tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos lectura del texto XML\n",
    "df_title_date_keywords_news_el_tiempo = pd.read_xml(StringIO(response.text), xpath=\".//news:news\", namespaces=namespaces)\n",
    "df_title_date_keywords_news_el_tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls_news_el_tiempo = (\n",
    "    df_urls_news_el_tiempo[[\"loc\"]]\n",
    "    .merge(\n",
    "        df_title_date_keywords_news_el_tiempo.drop(columns=[\"publication\"]),\n",
    "        left_index=True,\n",
    "        right_index=True        \n",
    "    )\n",
    "    .rename(columns={\"loc\": \"url_page\"})\n",
    ")\n",
    "\n",
    "df_urls_news_el_tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Obtener el texto de las noticias***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de url a visitar:\n",
    "df_urls_news_el_tiempo.iloc[0].loc[\"url_page\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Primeras 20 urls de noticias.\n",
    "list_urls = df_urls_news_el_tiempo[\"url_page\"].head(5).to_list()\n",
    "\n",
    "for url in list_urls:\n",
    "    print(\"Trabajando en la URL:\", url)\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    \n",
    "    # Verificar si el request es exitosa\n",
    "    if request.raise_for_status:\n",
    "        author_tag = soup.find(\"a\", class_=\"c-detail__author__name\").get_text()\n",
    "        content = \" \".join([tag.get_text() for tag in soup.find_all(\"div\", class_=\"paragraph\")])     \n",
    "        print(f\"Autor: {author_tag}\")\n",
    "        print(f\"Contenido noticia: {content}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Paralelizar la tarea de obtencion del contenido de las noticias***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_news_from_url(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Obtiene el contenido de una noticia desde una URL, extrayendo el autor y el cuerpo de la noticia.\n",
    "\n",
    "    :param url: URL de la página de la noticia.\n",
    "    :return: Diccionario con la URL, autor y contenido de la noticia.\n",
    "    :raises ValueError: Si hay un problema con el status_code de la solicitud.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    request = session.get(url)\n",
    "    \n",
    "    if request.raise_for_status:\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "        author_tag = soup.find(\"a\", class_=\"c-detail__author__name\").get_text()\n",
    "        content = \" \".join([tag.get_text() for tag in soup.find_all(\"div\", class_=\"paragraph\")])\n",
    "        \n",
    "        return {\n",
    "            \"url_page\": url,\n",
    "            \"autor\": author_tag,\n",
    "            \"news_content\": content            \n",
    "        }\n",
    "    else:\n",
    "        ValueError(\n",
    "            f\"Problemas con el status_code: {request.status_code}\"\n",
    "        )\n",
    "        \n",
    "\n",
    "get_content_news_from_url(df_urls_news_el_tiempo.iloc[0].loc[\"url_page\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Número de cores disponibles: {num_cores}\")\n",
    "\n",
    "# Numero de cores a usar.\n",
    "num_cores = num_cores - 2\n",
    "\n",
    "def process_url(url: str) -> Dict:\n",
    "    return get_content_news_from_url(url)\n",
    "\n",
    "\n",
    "def parallel_process_urls(urls: list[str]) -> list[Dict]:\n",
    "    \"\"\"\n",
    "    Procesa múltiples URLs en paralelo utilizando ThreadPoolExecutor,\n",
    "    dejando 2 cores libres.\n",
    "    \n",
    "    :param urls: Lista de URLs a procesar.\n",
    "    :return: Lista de diccionarios con la información extraída de cada URL.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
    "        futures = [executor.submit(process_url, url) for url in urls]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                results.append(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando la URL: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Lista de URLs a procesar.\n",
    "urls_to_process = df_urls_news_el_tiempo[\"url_page\"].tolist()\n",
    "# Procesamos todas las URLs en paralelo\n",
    "processed_results = parallel_process_urls(urls_to_process)\n",
    "# Resultados\n",
    "print(processed_results)\n",
    "pd.DataFrame(processed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_el_tiempo = df_urls_news_el_tiempo.merge(\n",
    "    pd.DataFrame(processed_results),\n",
    "    on=[\"url_page\"]    \n",
    ")\n",
    "\n",
    "display(df_news_el_tiempo)\n",
    "df_news_el_tiempo.to_parquet(\"data/df_noticias_el_tiempo.parquet\")\n",
    "pd.read_parquet(\"data/df_noticias_el_tiempo.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Usar un LLM local***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar Ollama (disponible para Windows, Linux y MacOS)\n",
    "# Servir (ollama serve)\n",
    "# Descargar los modelos:\n",
    "# 1. llama3.2:3b-instruct-q8_0 --> (ollama pull llama3.2:3b-instruct-q8_0)\n",
    "# 2. llama3.1:8b --> (ollama pull llama3.1:8b)\n",
    "# 3. qwen2.5:7b-instruct-q8_0 --> (ollama pull qwen2.5:7b-instruct-q8_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso es colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar un chat con un llm_ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm_ollama = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-q8_0\",\n",
    "    temperature=0.1,\n",
    "    num_predict=1024,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm_response = llm_ollama.invoke(\"¿Cuál es la segunda letra del alfabeto griego?\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm_ollama.invoke(\n",
    "    \"\"\"Dame un lista de 20 personajes influyentes en la actualidad\"\"\"\n",
    "    \"\"\"mundial. No olvides incluir diferentes angulos de la vida. \"\"\"\n",
    "    \"\"\"Solo debes darme la lista sin decir nada mas. Por ejemplo si \"\"\"\n",
    "    \"\"\"decides incluir a Pepito Perez (suponiendo que es influyente \"\"\"\n",
    "    \"\"\"actualmente) respuesta debe ser: Pepito Perez\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate y respuesta como string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"Eres una util AI bot que crea excelentes biografias de \"\"\"\n",
    "            \"\"\"personajes influyentes de la actualidad mundial y detalla su \"\"\"\n",
    "            \"\"\"vida en diferentes aspectos. Responde usando solo JSON con \"\"\"\n",
    "            \"\"\"llave el nombre del personaje y valor su biografia, \"\"\"\n",
    "            \"\"\"Por ejemplo: dict(Pepito Perez=\"Pepito Peres ... (biografia))\"\"\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "chain = prompt | llm_ollama | StrOutputParser()\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"user_input\": \"Barack Obama\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos JSON-format para parsear la respuesta del LLM\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b-instruct-q8_0\",\n",
    "    temperature=0.1,\n",
    "    num_predict=-1,\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"Eres una util AI bot que crea excelentes biografias de \"\"\"\n",
    "            \"\"\"personajes influyentes de la actualidad mundial y detalla su \"\"\"\n",
    "            \"\"\"vida ampliamente en diferentes aspectos. No debes escatimar \"\"\"\n",
    "            \"\"\"en la longitud del texto de resumen. Responde usando solo JSON \"\"\"\n",
    "            \"\"\"format con llave el nombre del personaje y valor su biografia\"\"\"\n",
    "            \"\"\"Por ejemplo: dict(Pepito Perez=\"Pepito Perez ... (biografia)\",\"\"\"\n",
    "            \"\"\" Benito Camelas=\"Benito Camelas ... (biografia)\"). Debes \"\"\"\n",
    "            \"\"\"seguir al pie de la letra el formato\"\"\"\n",
    "            ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | json_llm | JsonOutputParser()\n",
    "llm_answer = chain.invoke({\"user_input\": \"Dame la biografia de 2 personajes\"})\n",
    "print(llm_answer, type(llm_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.1:8b\",\n",
    ")\n",
    "\n",
    "question = [\"¿Cuál es la segunda letra del alfabeto griego?\"]\n",
    "embed_question = ollama_embeddings.embed_documents(question)\n",
    "print(\"Embeddings:\")\n",
    "pd.DataFrame(embed_question, index=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: (Relevancia - Distancia)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.1:8b\",\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"Alpha es la primera letra del alfabeto griego\",\n",
    "    \"A es la primera letra del alfabeto latino\",\n",
    "    \"Beta es la segunda letra del alfabeto griego\",\n",
    "    \"B es la segunda letra del alfabeto latino\"\n",
    "]\n",
    "query = [\"¿Cuál es la segunda letra del alfabeto griego?\"]\n",
    "embeds = ollama_embeddings.embed_documents(texts)\n",
    "embed_query = ollama_embeddings.embed_documents(query)\n",
    "print(f\"Long Ollama Embeddings: {len(embed_query[0])}\")\n",
    "print(f\"Cosine Similarities: \\n {cosine_similarity(embeds, embed_query)}\")\n",
    "\n",
    "pd.DataFrame(\n",
    "    cosine_similarity(embeds, embed_query),\n",
    "    index=texts,\n",
    "    columns=query\n",
    ").sort_values(by=query, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding de las noticias.\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.1:8b\",\n",
    ")\n",
    "\n",
    "df_temp = df_news_el_tiempo.sample(20)\n",
    "embed_news = ollama_embeddings.embed_documents(df_temp[\"news_content\"].to_list())\n",
    "print(\"Embeddings:\")\n",
    "pd.DataFrame(embed_news, index=df_temp[\"url_page\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***K-means sobre los embeddings***\n",
    "\n",
    "- Esta parte esta en construccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means sobre los embeddings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = StandardScaler().fit_transform(embed_news)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(scaled_data)\n",
    "df_temp[\"cluster\"] = kmeans.labels_\n",
    "display(df_temp[\"cluster\"].value_counts())\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos un modelo \"mas\" inteligente.\n",
    "llm_ollama = ChatOllama(\n",
    "    model=\"qwen2.5:7b-instruct-q8_0\",\n",
    "    temperature=0.1,\n",
    "    num_predict=-1,\n",
    "    num_ctx=32000\n",
    ")\n",
    "\n",
    "prompt_caracterizacion = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"Eres una util AI bot que identica las tematicas generales de \"\"\"\n",
    "            \"\"\"las noticias que te van a proporcionar pues estas noticias se \"\"\"\n",
    "            \"\"\"supone estas relacionadas por sus embeddings. La respuesta de los \"\"\"\n",
    "            \"\"\"temas deben ser similitudes precisamente semanticas, debes dar \"\"\"\n",
    "            \"\"\"tu respuesta con el titulo que determina porque son similares estas \"\"\"\n",
    "            \"\"\"noticias ademas de 5 bullets. Finalmente, daras un resumen al estilo \"\"\"\n",
    "            \"\"\"de historia de todas las noticias para poner al dia en noticias \"\"\"\n",
    "            \"\"\" de la tematicas que las hace coincidentes al usuario. Un resumen\"\"\"\n",
    "            \"\"\" por cada noticia.\"\"\"\n",
    "            \"\"\"Debes decirme cuantas noticias te fueron proporcionadas\"\"\"         \n",
    "            ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{news}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_chain = prompt_caracterizacion | llm_ollama | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "list_news = df_temp.query(\"cluster == 0\")[\"news_content\"].to_list()\n",
    "dict_news_content = {f\"noticia_{i+1}\": list_news[i] for i in range(len(list_news))}\n",
    "pprint(dict_news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json.dumps(dict_news_content)\n",
    "response_cluster_0 = llm_chain.invoke({\"news\": result})\n",
    "print(response_cluster_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
